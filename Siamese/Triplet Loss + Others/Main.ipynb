{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dweepa/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input\n",
    "from keras.layers.noise import AlphaDropout\n",
    "from keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# Import custom modules\n",
    "from network import *\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbfile = open('../../Data/full', 'rb')    \n",
    "data = pickle.load(dbfile)\n",
    "dbfile = open('../../Data/location_pert', 'rb')    \n",
    "location_pert = pickle.load(dbfile)\n",
    "dbfile = open('../../Data/pert2profiles', 'rb')    \n",
    "pert2profiles = pickle.load(dbfile)\n",
    "dbfile = open('../../Data/test_perts', 'rb')    \n",
    "test_pert = pickle.load(dbfile)\n",
    "dbfile = open('../../Data/train_perts', 'rb')    \n",
    "train_pert = pickle.load(dbfile)\n",
    "dbfile = open('../../Data/X_test_triplet', 'rb')\n",
    "X_test = pickle.load(dbfile)\n",
    "dbfile = open('../../Data/X_train_triplet', 'rb')\n",
    "X_train = pickle.load(dbfile)\n",
    "dbfile.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train = generate_data(data,train_pert[0:100],42) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_test = generate_data(data,test_pert[0:100],42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the base network\n",
    "network = dict(n_dense=5, dense_units=16, activation='selu', dropout=AlphaDropout, dropout_rate=0.1,\n",
    "               kernel_initializer='lecun_normal', optimizer='sgd', num_classes=2)\n",
    "\n",
    "shared_model = create_base_network(**network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred.shape =  Tensor(\"merged_layer/concat:0\", shape=(?, 48), dtype=float32)\n",
      "Tensor(\"loss/merged_layer_loss/Abs:0\", shape=(?,), dtype=float32)\n",
      "Tensor(\"loss/merged_layer_loss/Abs_1:0\", shape=(?,), dtype=float32)\n",
      "triplet network model summary\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor_input (InputLayer)       (None, 978)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive_input (InputLayer)     (None, 978)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative_input (InputLayer)     (None, 978)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 16)           16752       anchor_input[0][0]               \n",
      "                                                                 positive_input[0][0]             \n",
      "                                                                 negative_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "merged_layer (Concatenate)      (None, 48)           0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "                                                                 sequential_1[3][0]               \n",
      "==================================================================================================\n",
      "Total params: 16,752\n",
      "Trainable params: 16,752\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the siamese network\n",
    "model = create_siamese_network(shared_model)\n",
    "print(\"triplet network model summary\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = np.random.choice(X_train.shape[1],84, replace=False)  \n",
    "x_test_anch = X_train[0][index]\n",
    "x_test_pos = X_train[1][index]\n",
    "x_test_neg = X_train[2][index]\n",
    "\n",
    "def Diff(li1, li2):\n",
    "    li_dif = [i for i in li1 if i not in li1 or i not in li2]\n",
    "    return li_dif\n",
    "index2 = Diff([i for i in range(X_train.shape[1])], index)\n",
    "\n",
    "x_train_anch = X_train[0][index2]\n",
    "x_train_pos = X_train[1][index2]\n",
    "x_train_neg = X_train[2][index2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336, 978)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_anch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "336/336 [==============================] - 1s 4ms/step - loss: 5.8792\n",
      "Epoch 2/100\n",
      "336/336 [==============================] - 0s 172us/step - loss: 4.7468\n",
      "Epoch 3/100\n",
      "336/336 [==============================] - 0s 164us/step - loss: 4.8084\n",
      "Epoch 4/100\n",
      "336/336 [==============================] - 0s 174us/step - loss: 4.4842\n",
      "Epoch 5/100\n",
      "336/336 [==============================] - 0s 164us/step - loss: 3.8604\n",
      "Epoch 6/100\n",
      "336/336 [==============================] - 0s 171us/step - loss: 4.1187\n",
      "Epoch 7/100\n",
      "336/336 [==============================] - 0s 171us/step - loss: 3.9424\n",
      "Epoch 8/100\n",
      "336/336 [==============================] - 0s 172us/step - loss: 3.1798\n",
      "Epoch 9/100\n",
      "336/336 [==============================] - 0s 165us/step - loss: 3.3279\n",
      "Epoch 10/100\n",
      "336/336 [==============================] - 0s 173us/step - loss: 2.9847\n",
      "Epoch 11/100\n",
      "336/336 [==============================] - 0s 167us/step - loss: 2.8031\n",
      "Epoch 12/100\n",
      "336/336 [==============================] - 0s 170us/step - loss: 2.4715\n",
      "Epoch 13/100\n",
      "336/336 [==============================] - 0s 172us/step - loss: 2.6959\n",
      "Epoch 14/100\n",
      "336/336 [==============================] - 0s 169us/step - loss: 2.4507\n",
      "Epoch 15/100\n",
      "336/336 [==============================] - 0s 171us/step - loss: 2.4801\n",
      "Epoch 16/100\n",
      "336/336 [==============================] - 0s 171us/step - loss: 1.9726\n",
      "Epoch 17/100\n",
      "336/336 [==============================] - 0s 171us/step - loss: 1.9638\n",
      "Epoch 18/100\n",
      "336/336 [==============================] - 0s 169us/step - loss: 1.6093\n",
      "Epoch 19/100\n",
      "336/336 [==============================] - 0s 169us/step - loss: 1.5718\n",
      "Epoch 20/100\n",
      "336/336 [==============================] - 0s 158us/step - loss: 1.7268\n",
      "Epoch 21/100\n",
      "336/336 [==============================] - 0s 165us/step - loss: 1.3691\n",
      "Epoch 22/100\n",
      "336/336 [==============================] - 0s 165us/step - loss: 1.2799\n",
      "Epoch 23/100\n",
      "336/336 [==============================] - 0s 167us/step - loss: 1.3374\n",
      "Epoch 24/100\n",
      "336/336 [==============================] - 0s 172us/step - loss: 1.2028\n",
      "Epoch 25/100\n",
      "336/336 [==============================] - 0s 160us/step - loss: 1.0060\n",
      "Epoch 26/100\n",
      "336/336 [==============================] - 0s 169us/step - loss: 0.8424\n",
      "Epoch 27/100\n",
      "336/336 [==============================] - 0s 163us/step - loss: 0.7474\n",
      "Epoch 28/100\n",
      "336/336 [==============================] - 0s 164us/step - loss: 0.6700\n",
      "Epoch 29/100\n",
      "336/336 [==============================] - 0s 165us/step - loss: 0.6253\n",
      "Epoch 30/100\n",
      "336/336 [==============================] - 0s 162us/step - loss: 0.4265\n",
      "Epoch 31/100\n",
      "336/336 [==============================] - 0s 163us/step - loss: 0.4287\n",
      "Epoch 32/100\n",
      "336/336 [==============================] - 0s 165us/step - loss: 0.3962\n",
      "Epoch 33/100\n",
      "336/336 [==============================] - 0s 162us/step - loss: 0.2778\n",
      "Epoch 34/100\n",
      "336/336 [==============================] - 0s 171us/step - loss: 0.2060\n",
      "Epoch 35/100\n",
      "336/336 [==============================] - 0s 192us/step - loss: 0.2130\n",
      "Epoch 36/100\n",
      "336/336 [==============================] - 0s 419us/step - loss: 0.1896\n",
      "Epoch 37/100\n",
      "336/336 [==============================] - 0s 459us/step - loss: 0.1836\n",
      "Epoch 38/100\n",
      "336/336 [==============================] - 0s 337us/step - loss: 0.1511\n",
      "Epoch 39/100\n",
      "336/336 [==============================] - 0s 220us/step - loss: 0.1574\n",
      "Epoch 40/100\n",
      "336/336 [==============================] - 0s 244us/step - loss: 0.1251\n",
      "Epoch 41/100\n",
      "336/336 [==============================] - 0s 244us/step - loss: 0.1041\n",
      "Epoch 42/100\n",
      "336/336 [==============================] - 0s 172us/step - loss: 0.0944\n",
      "Epoch 43/100\n",
      "336/336 [==============================] - 0s 187us/step - loss: 0.0855\n",
      "Epoch 44/100\n",
      "336/336 [==============================] - 0s 184us/step - loss: 0.0759\n",
      "Epoch 45/100\n",
      "336/336 [==============================] - 0s 168us/step - loss: 0.0535\n",
      "Epoch 46/100\n",
      "336/336 [==============================] - 0s 176us/step - loss: 0.0697\n",
      "Epoch 47/100\n",
      "336/336 [==============================] - 0s 167us/step - loss: 0.0622\n",
      "Epoch 48/100\n",
      "336/336 [==============================] - 0s 190us/step - loss: 0.0478\n",
      "Epoch 49/100\n",
      "336/336 [==============================] - 0s 211us/step - loss: 0.0513\n",
      "Epoch 50/100\n",
      "336/336 [==============================] - 0s 224us/step - loss: 0.0591\n",
      "Epoch 51/100\n",
      "336/336 [==============================] - 0s 217us/step - loss: 0.0461\n",
      "Epoch 52/100\n",
      "336/336 [==============================] - 0s 224us/step - loss: 0.1238\n",
      "Epoch 53/100\n",
      "336/336 [==============================] - 0s 194us/step - loss: 0.0397\n",
      "Epoch 54/100\n",
      "336/336 [==============================] - 0s 191us/step - loss: 0.0378\n",
      "Epoch 55/100\n",
      "336/336 [==============================] - 0s 223us/step - loss: 0.0257\n",
      "Epoch 56/100\n",
      "336/336 [==============================] - 0s 187us/step - loss: 0.0211\n",
      "Epoch 57/100\n",
      "336/336 [==============================] - 0s 202us/step - loss: 0.0130\n",
      "Epoch 58/100\n",
      "336/336 [==============================] - 0s 193us/step - loss: 0.0307\n",
      "Epoch 59/100\n",
      "336/336 [==============================] - 0s 215us/step - loss: 0.0214\n",
      "Epoch 60/100\n",
      "336/336 [==============================] - 0s 196us/step - loss: 0.0186\n",
      "Epoch 61/100\n",
      "336/336 [==============================] - 0s 201us/step - loss: 0.0293\n",
      "Epoch 62/100\n",
      "336/336 [==============================] - 0s 243us/step - loss: 0.0086\n",
      "Epoch 63/100\n",
      "336/336 [==============================] - 0s 213us/step - loss: 0.0197\n",
      "Epoch 64/100\n",
      "336/336 [==============================] - 0s 231us/step - loss: 0.0250\n",
      "Epoch 65/100\n",
      "336/336 [==============================] - 0s 205us/step - loss: 0.0096\n",
      "Epoch 66/100\n",
      "336/336 [==============================] - 0s 200us/step - loss: 0.0058\n",
      "Epoch 67/100\n",
      "336/336 [==============================] - 0s 195us/step - loss: 0.0073\n",
      "Epoch 68/100\n",
      "336/336 [==============================] - 0s 195us/step - loss: 0.0121\n",
      "Epoch 69/100\n",
      "336/336 [==============================] - 0s 190us/step - loss: 0.0137\n",
      "Epoch 70/100\n",
      "336/336 [==============================] - 0s 219us/step - loss: 0.0061\n",
      "Epoch 71/100\n",
      "336/336 [==============================] - 0s 208us/step - loss: 0.0106\n",
      "Epoch 72/100\n",
      "336/336 [==============================] - 0s 194us/step - loss: 0.0038\n",
      "Epoch 73/100\n",
      "336/336 [==============================] - 0s 194us/step - loss: 0.0326\n",
      "Epoch 74/100\n",
      "336/336 [==============================] - 0s 202us/step - loss: 0.0142\n",
      "Epoch 75/100\n",
      "336/336 [==============================] - 0s 198us/step - loss: 0.0098\n",
      "Epoch 76/100\n",
      "336/336 [==============================] - 0s 194us/step - loss: 0.0017\n",
      "Epoch 77/100\n",
      "336/336 [==============================] - 0s 229us/step - loss: 0.0291\n",
      "Epoch 78/100\n",
      "336/336 [==============================] - 0s 194us/step - loss: 0.0078\n",
      "Epoch 79/100\n",
      "336/336 [==============================] - 0s 198us/step - loss: 0.0210\n",
      "Epoch 80/100\n",
      "336/336 [==============================] - 0s 195us/step - loss: 0.0049\n",
      "Epoch 81/100\n",
      "336/336 [==============================] - 0s 221us/step - loss: 0.0022\n",
      "Epoch 82/100\n",
      "336/336 [==============================] - 0s 196us/step - loss: 0.0154\n",
      "Epoch 83/100\n",
      "336/336 [==============================] - 0s 195us/step - loss: 0.0155\n",
      "Epoch 84/100\n",
      "336/336 [==============================] - 0s 193us/step - loss: 0.0101\n",
      "Epoch 85/100\n",
      "336/336 [==============================] - 0s 189us/step - loss: 0.0060\n",
      "Epoch 86/100\n",
      "336/336 [==============================] - 0s 214us/step - loss: 0.0047\n",
      "Epoch 87/100\n",
      "336/336 [==============================] - 0s 185us/step - loss: 0.0037\n",
      "Epoch 88/100\n",
      "336/336 [==============================] - 0s 189us/step - loss: 0.0014\n",
      "Epoch 89/100\n",
      "336/336 [==============================] - 0s 236us/step - loss: 0.0922\n",
      "Epoch 90/100\n",
      "336/336 [==============================] - 0s 209us/step - loss: 0.0012\n",
      "Epoch 91/100\n",
      "336/336 [==============================] - 0s 199us/step - loss: 0.0087\n",
      "Epoch 92/100\n",
      "336/336 [==============================] - 0s 208us/step - loss: 0.0108 0s - loss: 0.011\n",
      "Epoch 93/100\n",
      "336/336 [==============================] - 0s 215us/step - loss: 0.0018\n",
      "Epoch 94/100\n",
      "336/336 [==============================] - 0s 198us/step - loss: 0.0054\n",
      "Epoch 95/100\n",
      "336/336 [==============================] - 0s 223us/step - loss: 0.0068\n",
      "Epoch 96/100\n",
      "336/336 [==============================] - 0s 186us/step - loss: 0.0017\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336/336 [==============================] - 0s 197us/step - loss: 6.9142e-04\n",
      "Epoch 98/100\n",
      "336/336 [==============================] - 0s 176us/step - loss: 0.0178\n",
      "Epoch 99/100\n",
      "336/336 [==============================] - 0s 152us/step - loss: 0.0032\n",
      "Epoch 100/100\n",
      "336/336 [==============================] - 0s 154us/step - loss: 0.0103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a31a97d30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_dum = np.zeros((len(X_train[0]),1))\n",
    "#model.fit([X_train[0],X_train[1],X_train[2]],y_dum,epochs=100)\n",
    "y_dum = np.zeros((len(x_train_anch),1))\n",
    "model.fit([x_train_anch,x_train_anch,x_train_anch],y_dum,epochs=100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pred = model.predict([x_train_anch,x_train_anch,x_train_anch])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.583947 , -1.6194172, -1.5862105, ..., -1.536695 , -1.5743308,\n",
       "        -1.6507661],\n",
       "       [-1.5757152, -1.6363274, -1.5680358, ..., -1.5520461, -1.585606 ,\n",
       "        -1.6303561],\n",
       "       [-1.5731959, -1.6144158, -1.569976 , ..., -1.4694622, -1.5734942,\n",
       "        -1.7003508],\n",
       "       ...,\n",
       "       [-1.5529535, -1.6310512, -1.5700498, ..., -1.5251944, -1.5872489,\n",
       "        -1.6762955],\n",
       "       [-1.5860906, -1.6189401, -1.584371 , ...,  0.3392265,  1.3834811,\n",
       "         2.0342476],\n",
       "       [-1.5765158, -1.639289 , -1.5760888, ..., -1.5867943, -1.530628 ,\n",
       "        -1.5978836]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict([x_test_anch,x_test_pos,x_test_neg])\n",
    "pred"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pred = model.predict([X_test[0],X_test[1],X_test[2]])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "anch = normalize(pred[0][0:16].reshape(1,-1)).flatten()\n",
    "pos = normalize(pred[0][16:32].reshape(1,-1)).flatten()\n",
    "neg = normalize(pred[0][32:].reshape(1,-1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.999987, 0.9998356]\n",
      "[0.9998348, 0.9998255]\n",
      "[0.9998568, 0.99980605]\n",
      "[0.9998107, 0.9999363]\n",
      "[0.999984, 0.9998554]\n",
      "[0.9999171, 0.9999514]\n",
      "[0.9999774, 0.99967086]\n",
      "[0.999859, 0.99960554]\n",
      "[0.9998547, 0.9998702]\n",
      "[0.99994856, 0.9999565]\n",
      "[0.9999229, 0.9999172]\n",
      "[0.99997616, 0.9998562]\n",
      "[0.999959, 0.99997956]\n",
      "[0.9999441, 0.99986583]\n",
      "[0.9998956, 0.99998957]\n",
      "[0.99988127, 0.9999088]\n",
      "[0.9998465, 0.9999114]\n",
      "[0.9999581, 0.99991316]\n",
      "[0.99994946, 0.99989176]\n",
      "[0.9999647, 0.99978745]\n",
      "[0.99996585, 0.999953]\n",
      "[0.9999327, -0.8265517]\n",
      "[0.999984, 0.9999844]\n",
      "[0.99994826, 0.99992824]\n",
      "[0.99998224, 0.9998656]\n",
      "[0.99999666, 0.9997834]\n",
      "[0.99998295, 0.9998973]\n",
      "[0.99988127, 0.9997379]\n",
      "[0.99995166, 0.99979293]\n",
      "[0.99984515, 0.99990547]\n",
      "[0.9997624, 0.99963707]\n",
      "[0.9997292, 0.999798]\n",
      "[0.9998716, 0.99993145]\n",
      "[0.9997915, 0.999857]\n",
      "[0.9999031, 0.99998236]\n",
      "[0.9999031, 0.9998052]\n",
      "[0.99997365, 0.99963856]\n",
      "[0.99996346, 0.9998785]\n",
      "[0.9996818, 0.9998441]\n",
      "[0.99976176, 0.99969846]\n",
      "[0.9999489, 0.99995446]\n",
      "[0.99994767, 0.9998245]\n",
      "[0.99994576, 0.99987125]\n",
      "[0.99968207, 0.9998256]\n",
      "[0.999916, 0.9997637]\n",
      "[0.9999076, 0.9998525]\n",
      "[0.999984, 0.9999972]\n",
      "[0.9999209, 0.9998654]\n",
      "[0.9999712, 0.9999368]\n",
      "[0.99993753, 0.99880886]\n",
      "[0.9999499, 0.99998933]\n",
      "[0.99995327, 0.99980605]\n",
      "[0.9999769, 0.9999461]\n",
      "[0.9999486, 0.99993277]\n",
      "[0.99984396, 0.9999906]\n",
      "[0.9999445, 0.99991435]\n",
      "[0.9999332, 0.9999447]\n",
      "[0.9999704, 0.9999115]\n",
      "[0.99974763, 0.9999839]\n",
      "[0.99996424, 0.9999703]\n",
      "[0.9999373, 0.9998605]\n",
      "[0.9998691, 0.99981856]\n",
      "[0.9999569, 0.9998366]\n",
      "[0.9999736, 0.9998551]\n",
      "[0.9998926, 0.9989901]\n",
      "[0.99981856, 0.99998575]\n",
      "[0.999928, 0.9998709]\n",
      "[0.9997849, 0.99957144]\n",
      "[0.9998244, 0.99991256]\n",
      "[0.9998005, 0.9998886]\n",
      "[0.99995005, 0.9999884]\n",
      "[0.9998157, 0.9999074]\n",
      "[0.99989283, 0.9999417]\n",
      "[0.99980485, 0.9998281]\n",
      "[0.9998362, 0.99974036]\n",
      "[0.9999368, 0.99990404]\n",
      "[0.9993564, 0.9995898]\n",
      "[0.999967, 0.9996259]\n",
      "[0.9998911, 0.9999596]\n",
      "[0.99995553, 0.99996805]\n",
      "[0.9999815, 0.99989414]\n",
      "[0.9998844, 0.9999037]\n",
      "[0.9999857, -0.93609166]\n",
      "[0.99997103, 0.99976456]\n",
      "0.5119047619047619\n"
     ]
    }
   ],
   "source": [
    "values = []\n",
    "for i in pred:\n",
    "    anch = normalize(i[0:16].reshape(1,-1)).flatten()\n",
    "    pos = normalize(i[16:32].reshape(1,-1)).flatten()\n",
    "    neg = normalize(i[32:].reshape(1,-1)).flatten()\n",
    "    pos_neg_val = [(np.dot(anch,pos)), (np.dot(anch,neg))]\n",
    "    print(pos_neg_val)\n",
    "    values.append(pos_neg_val[0]>=0.7)\n",
    "    values.append(pos_neg_val[1]<0.7)\n",
    "    # print(positives, negatives)\n",
    "    #     print(pos_val, neg_val, pos_val>neg_val)\n",
    "print((np.sum(np.asarray(values)==True))/len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.99997103, 0.99976456]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_neg_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
